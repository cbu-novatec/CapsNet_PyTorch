{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the FashionMnist dataset\n",
    "The FashionMnist dataset is a direct replacement for the Mnist dataset. Containing images of clothes instead of numbers.\n",
    "The dataset is available here: https://github.com/zalandoresearch/fashion-mnist but can be directly downloaded through pytorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMnist:\n",
    "    def __init__(self, batch_size, transform = transforms.Compose([\n",
    "            transforms.ToTensor(), #convert image to PyTorch tensor\n",
    "            transforms.Normalize((0.1307,),(0.3081,)) # normalization \n",
    "        ]) ):\n",
    "        dataset_transform = transform\n",
    "        self.training_data = datasets.FashionMNIST('./data/fashionmnist', train = True, transform = dataset_transform, \n",
    "                                          download = True )\n",
    "        self.test_data = datasets.FashionMNIST('./data/fashionmnist', train = False,  transform = dataset_transform, \n",
    "                                      download = True )\n",
    "        \n",
    "        #batching and shuffeling data\n",
    "        self.train_loader  = torch.utils.data.DataLoader(self.training_data, batch_size=batch_size, shuffle=True)\n",
    "        self.test_loader  = torch.utils.data.DataLoader(self.test_data, batch_size=batch_size, shuffle=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I just plot some images from the dataset, to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26421880 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/fashionmnist/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26427392it [00:02, 10464176.05it/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/fashionmnist/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/fashionmnist/FashionMNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 269715.02it/s]                           \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/fashionmnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/fashionmnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/fashionmnist/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/fashionmnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4423680it [00:00, 8230176.80it/s]                            \n",
      "8192it [00:00, 111713.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/fashionmnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/fashionmnist/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/fashionmnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data/fashionmnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/fashionmnist/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "batch_size = 100\n",
    "mnist = FashionMnist(batch_size, transform = None)\n",
    "\n",
    "labels_map = {0 : 'T-Shirt', 1 : 'Trouser', 2 : 'Pullover', 3 : 'Dress', 4 : 'Coat', 5 : 'Sandal', 6 : 'Shirt',\n",
    "              7 : 'Sneaker', 8 : 'Bag', 9 : 'Ankle Boot'};\n",
    "\n",
    "fig = plt.figure(figsize=(8,8));\n",
    "columns = 4;\n",
    "rows = 5;\n",
    "\n",
    "for i in range(1, columns*rows +1):\n",
    "    img_xy = np.random.randint(len(mnist.training_data));\n",
    "    img = mnist.training_data[img_xy][0]\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.title(labels_map[mnist.training_data[img_xy][1]])\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are the class labels:\n",
    "<div>\n",
    "<img src=\"figures\\image.png\" width = \"180\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"figures\\architecture_encoder.gif\" width = \"880\"/>\n",
    "</div>\n",
    "This is the architecture for the MNIST dataset proposed by Sabour et al. (2017). There is a convolutional layer first, followed by two capsule layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layer\n",
    "**Input:** 28x28x1 (image size and grey scale channel)\n",
    "\n",
    "**Output:** 20x20x256\n",
    "\n",
    "The first layer is a convolutional layer and extracts basic features. The images in the FashionMnist dataset have an input size of 28x28 pixels. We convolve with 256 kernels of size 9x9 over the input image. This results in 256 output images of size 20x20 (the output size of a convolutional network is $(input size - kernel size)+1$.\n",
    "We then apply a Rectified Linear Unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# parameters taken from paper\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=256, kernel_size=9):\n",
    "        super(ConvLayer, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels,\n",
    "                               out_channels=out_channels,\n",
    "                               kernel_size=kernel_size,\n",
    "                               stride=1\n",
    "                             )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Capsule Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input:** 20x20x256\n",
    "\n",
    "**Output:** 1152x8\n",
    "\n",
    "So now, instead of having a pooling layer, we get to our first capsule layer. Well, this isn't really a proper capsule layer, but actually another convolutional layer. The only new thing we do here, is to *reshape* and *squash*.\n",
    "\n",
    "This time we convolve with kernels of size 9x9x256, as we have a stack of 256 outputs from before. The stride this time is 2. We then get 256 output images of size 6x6 (this time $\\frac{(inputsize - kernelsize) + 1}{2}$, as we have stride 2).\n",
    "We then can then *reshape* this stack by splitting it into 32 decks, each containing 8 output images of size 6x6. \n",
    "<div>\n",
    "<img src=\"figures\\capsule_layer.gif\" width = \"500\"/>\n",
    "</div>\n",
    "We get in each deck 36 capsules, and 32 decks in total. This amounts to $36 * 32 = 1152$  capsules.\n",
    "The last step is to squash those capsules.\n",
    "Remember the squash function:\n",
    "\n",
    "$$v_j = \\frac{\\|s_j\\|^2 }{1+\\|s_j\\|^2} \\frac{s_j}{\\|s_j\\|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    # perform depthwise convolution\n",
    "    def __init__(self, in_channels = 256 , out_channels = 256 , kernel_size = 9, groups = 256, stride = 2):\n",
    "        super(PrimaryCaps,self).__init__()\n",
    "        self.primary_caps = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                                  kernel_size = kernel_size, stride = stride)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        u = self.primary_caps(x)\n",
    "        u = u.view(x.size(0), 32 * 6 * 6, -1) \n",
    "        return self.squash(u)\n",
    "\n",
    "    def squash(self, tensor):\n",
    "        squared_norm = (tensor ** 2).sum(-1, keepdim=True)\n",
    "        scaled = squared_norm/(1+squared_norm)\n",
    "        return scaled*tensor/torch.sqrt(squared_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Caps Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input:** 1152x8\n",
    "\n",
    "**Output:** 10x16\n",
    "\n",
    "Now we have a \"proper\" capsule layer.\n",
    "We have a vector input (of size 1x8). Those inputs get multiplied with affine transformation matrices W. On the transformed vectors we then use routing by agreement. \n",
    "For each input vector we determine the coupling coefficient to the next layer capsule. As we have 10 classes, we have 10 capsules to route to. Each capsules has a size of 16, which is the choice they made in the paper. The length of each of those 10 16-dimensional vectors, gives us the confidence for the corresponding class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=10, previous_capsules=32 * 6 * 6, in_channels=8, out_channels=16):\n",
    "        super(SecondCaps, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.previous_capsules = previous_capsules\n",
    "        self.num_capsules = num_capsules\n",
    "\n",
    "        self.W = nn.Parameter(torch.randn(1, previous_capsules, num_capsules, out_channels, in_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.stack([x] * self.num_capsules, dim=2).unsqueeze(4)\n",
    "\n",
    "        W = torch.cat([self.W] * batch_size, dim=0) #\n",
    "        u_hat = torch.matmul(W, x) #W*x = u_hat\n",
    "\n",
    "        num_iterations = 3  # taken from paper\n",
    "        v_j = self.routing_by_agreement(u_hat, num_iterations)\n",
    "\n",
    "        return v_j.squeeze(1)\n",
    "    \n",
    "    def routing_by_agreement(self, u_hat, num_iterations):\n",
    "        b_ij = torch.zeros(1, self.previous_capsules, self.num_capsules, 1) #initialize b_ij = 0\n",
    "        for iteration in range(num_iterations): \n",
    "            c_ij = F.softmax(b_ij, dim = 1) # scale each capsules weights so they sum to 1\n",
    "            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n",
    "            # transformed vectors get multiplied element-wise with the coupling coefficients\n",
    "            # for each high level capsule, the sum over all lower level capsules inputs is calculated\n",
    "            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True) \n",
    "            v_j = self.squash(s_j) # normalize with squash function\n",
    "            # don't need to calculate new b_ij in last iteration   \n",
    "            if iteration < num_iterations - 1:\n",
    "                a_ij = torch.matmul(u_hat.transpose(3, 4), torch.cat([v_j] * self.previous_capsules, dim=1))\n",
    "                b_ij = b_ij + a_ij.mean(dim=0, keepdim=True).squeeze(4)\n",
    "        return v_j\n",
    "    \n",
    "    def squash(self, tensor):\n",
    "        squared_norm = (tensor ** 2).sum(-1, keepdim=True)\n",
    "        scaled = squared_norm/(1+squared_norm)\n",
    "        return scaled*tensor/torch.sqrt(squared_norm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction\n",
    "\n",
    "The next layer reconstructs the image from the instantiation parameters given by the capsule output.\n",
    "<div>\n",
    "<img src=\"figures\\Decoder.png\" width = \"700\"/>\n",
    "</div>\n",
    "\n",
    "It is a simple fully connected network. It is later used for regularization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_capsule_param = 16, num_capsules=10, num_hidden = 512):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.reconstruction_layers = nn.Sequential(\n",
    "            nn.Linear(num_capsule_param* num_capsules, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(num_hidden, num_hidden*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(num_hidden*2, 28 * 28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, data):\n",
    "        # calculate length for each capsule\n",
    "        caps_classes = torch.sqrt((x ** 2).sum(2))\n",
    "        caps_classes = F.softmax(caps_classes, dim = 1)\n",
    "        # find capsule with maximum vector length, reflecting highest probability\n",
    "        max_indices = caps_classes.argmax(dim=1)\n",
    "        mask_matrix =torch.eye(10)\n",
    "        mask_matrix = mask_matrix.index_select(dim=0, index=max_indices.squeeze(1).data)\n",
    "        y = (x * mask_matrix[:, :, None, None]).view(x.size(0), -1)\n",
    "        reconstructions = self.reconstruction_layers(y)\n",
    "        \n",
    "        reconstructions = reconstructions.view(-1, 1, 28, 28)\n",
    "\n",
    "        return reconstructions, mask_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "Now we have all our layers and we can put them together.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.conv_layer = ConvLayer()\n",
    "        self.primary_capsules = PrimaryCaps()\n",
    "        self.second_capsules = SecondCaps()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "     \n",
    "    def forward(self, data):\n",
    "        output = self.second_capsules(self.primary_capsules(self.conv_layer(data)))\n",
    "        reconstructions, masked = self.decoder(output, data)\n",
    "        return output, reconstructions, masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "Our loss function is the sum of of the margin loss and the reconstruction loss.\n",
    "\n",
    "The paper uses a special margin loss to make it possible to detect two or more different digits in each image:\n",
    "\n",
    "$$L_k = T_k \\max(0, m^{+} - \\|\\mathbf{v}_k\\|)^2 + \\lambda (1 - T_k) \\max(0, \\|\\mathbf{v}_k\\| - m^{-})^2$$\n",
    "\n",
    "$T_k$ is equal to 1 if the digit of class $k$ is present, or 0 otherwise.\n",
    "In the paper, $m^{+} = 0.9$, $m^{-} = 0.1$ and $\\lambda = 0.5$.\n",
    "\n",
    "The reconstruction loss is the euclidean distance between the reconstructed and the original image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsuleLoss,self).__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "            \n",
    "    # our loss is margin_loss + reconstruction_loss\n",
    "    # the reconstruction loss is multiplied with 0.0005 to ensure that the margin loss dominates the total loss\n",
    "    def forward(self, data, x, target, reconstructions):\n",
    "        return self.margin_loss(x, target) + (0.0005*self.reconstruction_loss(data, reconstructions))\n",
    "    \n",
    "    # special margin_loss definied in the paper\n",
    "    def margin_loss(self, x, labels,lamda = 0.5, size_average=True):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        v_c_norm = torch.sqrt((x**2).sum(dim=2, keepdim=True))\n",
    "\n",
    "        left = ((F.relu(0.9 - v_c_norm))**2).view(batch_size, -1)\n",
    "        right = ((F.relu(v_c_norm - 0.1))**2).view(batch_size, -1)\n",
    "\n",
    "        loss = labels * left + lamda * (1.0 - labels) * right\n",
    "        loss = loss.sum(dim=1).mean()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    # squared difference between reconstructed and original image\n",
    "    def reconstruction_loss(self, data, reconstructions):\n",
    "        loss = self.mse_loss(reconstructions.view(reconstructions.size(0), -1), data.view(reconstructions.size(0), -1))\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "\n",
    "capsule_net = CapsNet()\n",
    "capsule_loss = CapsuleLoss()\n",
    "# optimizer used in the paper\n",
    "optimizer = Adam(capsule_net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Train accuracy: 0.08\n",
      "Epoch: 0 \t Train accuracy: 0.82\n",
      "Epoch: 0 \t Train accuracy: 0.83\n",
      "Epoch: 0 \t Train accuracy: 0.82\n",
      "Epoch: 0 \t Train accuracy: 0.83\n",
      "Epoch: 0 \t Train accuracy: 0.89\n",
      "Training Loss: 0.17198263108730316\n",
      "Epoch: 0 \t Test accuracy: 0.87\n",
      "Test Loss: 0.1036573126912117\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1 # the more epochs, the longer the training, but better results. Try what works on your computer.\n",
    "batch_size = 100\n",
    "mnist = FashionMnist(batch_size)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    capsule_net.train()\n",
    "    train_loss = 0\n",
    "    for batch_id, (data, target) in enumerate(mnist.train_loader):\n",
    "\n",
    "        target = torch.eye(10).index_select(dim=0, index=target)\n",
    "        data, target = data, target\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, reconstructions, masked = capsule_net(data)\n",
    "        loss = capsule_loss(data, output, target, reconstructions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.data\n",
    "        \n",
    "        if batch_id % 100 == 0:\n",
    "            accuracy = sum(np.argmax(masked.data.cpu().numpy(), 1) == \n",
    "                           np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size)\n",
    "            print (\"Epoch: {} \\t Train accuracy: {}\".format(epoch, accuracy))\n",
    "       \n",
    "    print (\"Training Loss: {}\".format(train_loss / len(mnist.train_loader)))\n",
    "        \n",
    "    capsule_net.eval()\n",
    "    test_loss = 0\n",
    "    for batch_id, (data, target) in enumerate(mnist.test_loader):\n",
    "\n",
    "        target = torch.eye(10).index_select(dim=0, index=target)\n",
    "        data, target = data, target\n",
    "\n",
    "        output, reconstructions, masked = capsule_net(data)\n",
    "        loss = capsule_loss(data, output, target, reconstructions)\n",
    "\n",
    "        test_loss += loss.data\n",
    "        \n",
    "        if batch_id % 100 == 0:\n",
    "            accuracy = sum(np.argmax(masked.data.cpu().numpy(), 1) == \n",
    "                                   np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size)\n",
    "            print (\"Epoch: {} \\t Test accuracy: {}\".format(epoch,accuracy) )\n",
    "    \n",
    "    print(\"Test Loss: {}\".format(test_loss / len(mnist.test_loader)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructed images\n",
    "\n",
    "To see how good the network works, we can plot the reconstructed images and compare them to the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images):\n",
    "    fig = plt.figure()\n",
    "    for j in range(1, 7):\n",
    "        ax = fig.add_subplot(1, 6, j)\n",
    "        ax.matshow(images[j-1], cmap = matplotlib.cm.binary)\n",
    "        plt.xticks(np.array([]))\n",
    "        plt.yticks(np.array([]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images(data[:6,0].data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images(reconstructions[:6,0].data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/gram-ai/capsule-networks\n",
    "\n",
    "https://github.com/higgsfield/Capsule-Network-Tutorial/blob/master/Capsule%20Network.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
